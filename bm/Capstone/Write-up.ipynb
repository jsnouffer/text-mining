{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A brief introduction\n",
    "\n",
    "MBTI, short for Myers-Briggs Type Indicator, is a personality metric developed by Katharine Cook Briggs and her daughter Isabel Briggs Myers, based on Carl Jung's theory on psychological types. Today, it is a common tool used by individuals and organizations alike, be it to better understand themselves or to optimize workplace dynamics.\n",
    "\n",
    "For those who still do not know about MBTI:\n",
    "\n",
    "Each person is tested in 4 different areas:\n",
    "\n",
    "![mbtimodel](mbtimodel.jpg)\n",
    "source: CPP Inc.\n",
    "\n",
    "Each person would be typed with 4 letters according to MBTI. So for example for someone whose type is ENFJ, this means that this person is extraverted, intuitive, feeling and judging. I hope this little write-up is succinct enough, but if it is not then please feel free to refer to the official [MBTI page](http://www.myersbriggs.org/my-mbti-personality-type/mbti-basics/home.htm?bhcp=1) to find out more. \n",
    "\n",
    "The most common way of finding out of our type is to visit free personality test websites, where they would require you to answer questions after questions after questions (or statements) in order to determine your type, as accurately as possible. More often than not, these questions relate directly to the type characteristic which requires you to rate how well you 'relate' to the question asked. For example:\n",
    "\n",
    "> 25. My idea of relaxation involves reading a book by the beach\n",
    "> \n",
    " * Strongly Disagree\n",
    " * Disagree\n",
    " * Neutral\n",
    " * Agree\n",
    " * Strongly Agree\n",
    "\n",
    "If you haven't noticed yet, this presents a series of problems, in no order of magnitude:\n",
    "\n",
    "1. We'd inherently know that this question refers to Extroversion/Introversion, and hence may tend to answer based on how we identify rather than purely relating to the question asked.\n",
    "    * In other words, we'd answer with some form of a bias i.e. Who we think we are or want to be vs who we really are.\n",
    "    * Come to think of it, regarding the spelling of extrovert or extravert (oh good lord spellcheck got activated for the latter!), I found [this](https://blogs.scientificamerican.com/beautiful-minds/the-difference-between-extraversion-and-extroversion/).\n",
    "2. I identify as a strong introvert but I don't really dig reading a book by the beach, nor do people from landlocked areas etc...\n",
    "3. We cannot, in some senses, identify by how much we agree/disagree with the statement/question. However, in order for the model to work, we have to choose a side. Strongly.\n",
    "4. Answering so many questions is already by itself a big time waster, not to mention it being a tiring process.\n",
    "\n",
    "The question wasn't picked up from any site in particular by the way, I made it up!\n",
    "\n",
    "## Proposal\n",
    "\n",
    "My project shall attempt to aid users in having a seamless experience in finding out their MBTI type. Instead of having the user dedicate his/her precious time and brain energy to processing all the questions, the machine only needs to pick up the existing messages produced by the user to predict their MBTI type!\n",
    "\n",
    "Read on if you would like to understand the how, but beware, it can get a little technical. Otherwise, click [here](https://yix90.github.io) to go straight to the webapp for some fun!\n",
    "\n",
    "### How\n",
    "The model makes use of forum posts from personalitycafe.com for training. Luckily for me, this dataset is already made available on Kaggle in the form of 50 posts per person of a certain MBTI type. Not a competition piece though, just a dataset for us to play around with.\n",
    "\n",
    "The dataset only consists of two columns, one being the type and the other being the collection of posts made by the person of the type. As such, therein begins my journey of digging out and processing the data from the text followed by modelling and predicting.\n",
    "\n",
    "Here is a preview of the first line:\n",
    "\n",
    "--------------------------------------------\n",
    "\n",
    "type: \n",
    "``` \n",
    "INFJ\n",
    "```\n",
    "\n",
    "posts: \n",
    "``` \n",
    "\"'http://www.youtube.com/watch?v=qsXHcwe3krw|||http://41.media.tumblr.com/tumblr_lfouy03PMA1qa1rooo1_500.jpg|||enfp and intj moments https://www.youtube.com/watch?v=iz7lE1g4XM4 sportscenter not top ten plays https://www.youtube.com/watch?v=uCdfze1etec pranks|||What has been the most life-changing experience in your life?|||http://www.youtube.com/watch?v=vXZeYwwRDw8 http://www.youtube.com/watch?v=u8ejam5DP3E On repeat for most of today.|||May the PerC Experience immerse you.|||The last thing my INFJ friend posted on his facebook before committing suicide the next day. Rest in peace~ http://vimeo.com/22842206|||Hello ENFJ7. Sorry to hear of your distress. It's only natural for a relationship to not be perfection all the time in every moment of existence. Try to figure the hard times as times of growth, as...|||84389 84390 http://wallpaperpassion.com/upload/23700/friendship-boy-and-girl-wallpaper.jpg http://assets.dornob.com/wp-content/uploads/2010/04/round-home-design.jpg ...|||Welcome and stuff.|||http://playeressence.com/wp-content/uploads/2013/08/RED-red-the-pokemon-master-32560474-450-338.jpg Game. Set. Match.|||Prozac, wellbrutin, at least thirty minutes of moving your legs (and I don't mean moving them while sitting in your same desk chair), weed in moderation (maybe try edibles as a healthier alternative...|||Basically come up with three items you've determined that each type (or whichever types you want to do) would more than likely use, given each types' cognitive functions and whatnot, when left by...|||All things in moderation. Sims is indeed a video game, and a good one at that. Note: a good one at that is somewhat subjective in that I am not completely promoting the death of any given Sim...|||Dear ENFP: What were your favorite video games growing up and what are your now, current favorite video games? :cool:|||https://www.youtube.com/watch?v=QyPqT8umzmY|||It appears to be too late. :sad:|||There's someone out there for everyone.|||Wait... I thought confidence was a good thing.|||I just cherish the time of solitude b/c i revel within my inner world more whereas most other time i'd be workin... just enjoy the me time while you can. Don't worry, people will always be around to...|||Yo entp ladies... if you're into a complimentary personality,well, hey.|||... when your main social outlet is xbox live conversations and even then you verbally fatigue quickly.|||http://www.youtube.com/watch?v=gDhy7rdfm14 I really dig the part from 1:46 to 2:50|||http://www.youtube.com/watch?v=msqXffgh7b8|||Banned because this thread requires it of me.|||Get high in backyard, roast and eat marshmellows in backyard while conversing over something intellectual, followed by massages and kisses.|||http://www.youtube.com/watch?v=Mw7eoU3BMbE|||http://www.youtube.com/watch?v=4V2uYORhQOk|||http://www.youtube.com/watch?v=SlVmgFQQ0TI|||Banned for too many b's in that sentence. How could you! Think of the B!|||Banned for watching movies in the corner with the dunces.|||Banned because Health class clearly taught you nothing about peer pressure.|||Banned for a whole host of reasons!|||http://www.youtube.com/watch?v=IRcrv41hgz4|||1) Two baby deer on left and right munching on a beetle in the middle. 2) Using their own blood, two cavemen diary today's latest happenings on their designated cave diary wall. 3) I see it as...|||a pokemon world an infj society everyone becomes an optimist|||49142|||http://www.youtube.com/watch?v=ZRCEq_JFeFM|||http://discovermagazine.com/2012/jul-aug/20-things-you-didnt-know-about-deserts/desert.jpg|||http://oyster.ignimgs.com/mediawiki/apis.ign.com/pokemon-silver-version/d/dd/Ditto.gif|||http://www.serebii.net/potw-dp/Scizor.jpg|||Not all artists are artists because they draw. It's the idea that counts in forming something of your own... like a signature.|||Welcome to the robot ranks, person who downed my self-esteem cuz I'm not an avid signature artist like herself. :proud:|||Banned for taking all the room under my bed. Ya gotta learn to share with the roaches.|||http://www.youtube.com/watch?v=w8IgImn57aQ|||Banned for being too much of a thundering, grumbling kind of storm... yep.|||Ahh... old high school music I haven't heard in ages. http://www.youtube.com/watch?v=dcCRUPCdB1w|||I failed a public speaking class a few years ago and I've sort of learned what I could do better were I to be in that position again. A big part of my failure was just overloading myself with too...|||I like this person's mentality. He's a confirmed INTJ by the way. http://www.youtube.com/watch?v=hGKLI-GEc6M|||Move to the Denver area and start a new life for myself.'\"\n",
    "```\n",
    "-------------------------------------------\n",
    "\n",
    "Wow. Imagine 8675 times that chunk of text!\n",
    "\n",
    "\n",
    "### TL;DR\n",
    "If, at this point, you are already feeling afraid of what is to come, here is pretty much a summary of what I did with the dataset:\n",
    "\n",
    "* Basic data cleaning (or the lack thereof - its actually quite clean already)\n",
    "* Extract weblinks from each user, and\n",
    "  * Further divide the weblinks into video, images and others\n",
    "  * For videos, extract video titles from the respective sites (not used)\n",
    "  * For images, extract keywords (not done)\n",
    "  * For other websites, obtain categories (not done)\n",
    "* Split the target data from 16 categories into 4 binary classifiers (The why will be explained below)\n",
    "* Extract other metadata:\n",
    "  * Emoticons (eg. :happy:)\n",
    "  * Mentions (eg. @tamagonii)\n",
    "  * Hashtags (eg. #ilovetamago)\n",
    "  * MBTI reference (eg. INFP, ENFJ) (not used)\n",
    "  * Action words (eg. \\*jumps into the pool and swim away\\*)\n",
    "  * Enneagram regerence (eg. 4w1) (not used)\n",
    "  * Brackets (eg. [quote])\n",
    "  * Dots count (...)\n",
    "  * Number of words\n",
    "  * Number of word characters\n",
    "  * Number of fully capitalized words (eg. HEY Y'ALL!!)\n",
    "  * Number of characters of fully capitalized words\n",
    "  * Ratio of fully capitalized words vs all words\n",
    "  * Ratio of characters of fully capitalized words vs characters of all words\n",
    "  * Median number of words used per person\n",
    "  * Median number of characters used per person\n",
    "* Perform Parts-of-speech (POS) tagging to the word document\n",
    "* For each MBTI type,\n",
    "  * Perform Term Frequency - Inverse Document Frequency (TF-IDF)\n",
    "    * For word range of 1-3, up to 10,000 words/phrases each\n",
    "    * For each word, apply Truncated Singular Value Decomposition (Truncated SVD) to reduce the size to 500 features each, totalling 1500 features\n",
    "  * Pre-process the data:\n",
    "    * Perform Standard Scaling on metadata\n",
    "    * Combine with TFIDF data, and perform MinMax Scaling\n",
    "  * Under-sample the majority class for imbalanced datasets\n",
    "  * Select 100 best features using chi2 test\n",
    "  * Train the model with Logistic Regression\n",
    "* Collect instances of all 4 types to predict data from new input data\n",
    "* Done!\n",
    "\n",
    "If you haven't noticed already, I used some 'big' words to describe the process. If you would like to learn what they mean and what they do, read on below!\n",
    "\n",
    "## The ~~boring~~ important stuff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Congratulations for making to this point. From here on, I shall go into a little more detail, the steps taken towards greatness.\n",
    "\n",
    "### Data observation and cleaning\n",
    "\n",
    "As mentioned, the dataset comes with just two columns: The MBTI type itself and 50 posts made by the person of the said MBTI type. \n",
    "\n",
    "We have:\n",
    "* No null values\n",
    "* Shape = (8675, 2)\n",
    "* Nothing else significant to note (for now)\n",
    "\n",
    "### Target variable\n",
    "\n",
    "As with any Machine Learning tasks, we must first define our target variable. There are a total of 16 different combinations of MBTI types, which means we would have a 16-class classification problem to solve\n",
    "\n",
    "...or do we?\n",
    "\n",
    "Here is a breakdown on the number of people per MBTI type (in yellow), benchmarked against the global population percentage representation of each type mapped onto the graph (taken from [careerplanner.com](https://www.careerplanner.com/MB2/TypeInPopulation.cfm)):\n",
    "\n",
    "![mbti_comparison](mbti_comparison.png)\n",
    "\n",
    "And here is the breakdown of the total number of people of each type in the dataset:\n",
    "\n",
    "```\n",
    "INFP    1832\n",
    "INFJ    1470\n",
    "INTP    1304\n",
    "INTJ    1091\n",
    "ENTP     685\n",
    "ENFP     675\n",
    "ISTP     337\n",
    "ISFP     271\n",
    "ENTJ     231\n",
    "ISTJ     205\n",
    "ENFJ     190\n",
    "ISFJ     166\n",
    "ESTP      89\n",
    "ESFP      48\n",
    "ESFJ      42\n",
    "ESTJ      39\n",
    "```\n",
    "\n",
    "On first glance this would look like a very bleak start to the project since it is heavily imbalanced...good luck to me.\n",
    "\n",
    "**\\*Ok actually I can fix this\\***\n",
    "\n",
    "Instead of doing 16 imbalanced classification head-on, this dataset can be re-classified as 4 binary classifiers!\n",
    "\n",
    "```python\n",
    "mbtitypes_all['is_E'] = mbtitypes_all['type'].apply(lambda x: 1 if x[0] == 'E' else 0)\n",
    "mbtitypes_all['is_S'] = mbtitypes_all['type'].apply(lambda x: 1 if x[1] == 'S' else 0)\n",
    "mbtitypes_all['is_T'] = mbtitypes_all['type'].apply(lambda x: 1 if x[2] == 'T' else 0)\n",
    "mbtitypes_all['is_J'] = mbtitypes_all['type'].apply(lambda x: 1 if x[3] == 'J' else 0)\n",
    "mbtitypes_all.columns = ['type','is_E','is_S','is_T','is_J']\n",
    "mbtitypes_all.head()\n",
    "```\n",
    "type|is_E|is_S|is_T|is_J\n",
    "----|----|----|----|----\n",
    "INFJ|0|0|0|1\n",
    "ENTP|1|0|1|0\n",
    "INTP|0|0|1|0\n",
    "INTJ|0|0|1|1\n",
    "ENTJ|1|0|1|1\n",
    "\n",
    "Here we visualize the classifiers once again:\n",
    "\n",
    "![4_piechart](4_piechart.png)\n",
    "\n",
    "This looks a lot better than the previous 16-class variable, though we still see imbalanced targets for E/I and S/N. We shall deal with them in due time.\n",
    "\n",
    "**Assumption made:** Each letter type is independent of other types i.e. A person's introversion/extraversion is totally not related to their judgement/perception. Nevertheless, we can still test them:\n",
    "\n",
    "Correlation - How close each feature is affected by another. \n",
    "\n",
    "For example, if the amount of sales drops/increases definitely with an increase in price, we can say that amount of sales and price are correlated. Whereas in the case where the number of shoppers does not increase/decrease significantly with the changes in pricing, we can say that they have little to no correlation.\n",
    "\n",
    "![4type_corr](4type_corr.png)\n",
    "\n",
    "The correlation for all 4 types are very close to 0, which is a good sign.\n",
    "\n",
    "### Features\n",
    "\n",
    "Time to extract our features!\n",
    "\n",
    "We first have a cursory look at the text data, which we will see that it is made up of more than mere text alone:\n",
    "\n",
    "##### Webpages\n",
    "\n",
    "* Video links: https://www.youtube.com/watch?v=QyPqT8umzmY\n",
    "* Image links: http://41.media.tumblr.com/tumblr_lfouy03PMA1qa1rooo1_500.jpg\n",
    "* Other links: http://phantomshine.blogspot.com/2012/05/writer-analysis-through-mbti.html (taken from another data point)\n",
    "I'll get to the other data (as mentioned in TLDR) later, I promise.\n",
    "\n",
    "Video, image and other links can potentially add to the dataset, other than their mere count. We can get video titles from video links, perform image content analysis for image links, or simply category identifying for other links. All these data can potentially be used for topic modelling to find out what kind of topics do people of a certain MBTI type care about collectively that the other type does not care as much about.\n",
    "\n",
    "Unfortunately I only went as far as extract the video title and nothing else. Sorry... :sob:\n",
    "\n",
    "##### Other Metadata\n",
    "\n",
    "Once again, you may notice other non-regular text like @mentions, #tags, \\*emphasis or action words\\*, :emoticons:, [bracket words], even dot usage (...) etc. For those features I extracted the total count for those features. On hindsight I could have gotten the mean instead, but it was already too late for that decision stage. I managed to 'fix' it later on.\n",
    "\n",
    "I also extracted the number of MBTI mentions (since the dataset was from a personality discussion forum) and Enneagram references. Sidetrack - The Enneagram is another form of personality typology which could potentially serve as my next project, but not now. Priorities. Meanwhile, you may also take the [test](https://www.eclecticenergies.com/enneagram/test) if you are interested :wink:\n",
    "\n",
    "Other data available includes word count, character count, number of fully capitalized words (that are not MBTI references) etc which are also incorporated as features.\n",
    "\n",
    "Here's a cursory view of the dataframe after the processing:\n",
    "\n",
    "![df_meta](df_meta.png)\n",
    "\n",
    "### Parts of Speech (POS) tagging\n",
    "\n",
    "Parts of Speech or POS tagging is used for identifying the type of words within a sentence. For my use case, the area of interest would be to find out once again if there is any discrimination between the frequency of the type of words used between each MBTI type. I did the POS tagging using the Natural Language ToolKit (NLTK) POS tagger.\n",
    "\n",
    "```python\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#Takes a long time to run!\n",
    "mbti['tagged_words'] = mbti['words_only'].apply(\n",
    "    lambda x: [nltk.pos_tag(word_tokenize(line.decode('utf-8', errors='replace'))) for line in x])\n",
    "mbti['tagged_words'][0]\n",
    "```\n",
    "Output of first 5 lines (First 2 lines contain only links which are promptly removed):\n",
    "\n",
    "```\n",
    "[[],\n",
    " [],\n",
    " [(u'enfp', 'NN'),\n",
    "  (u'and', 'CC'),\n",
    "  (u'intj', 'JJ'),\n",
    "  (u'moments', 'NNS'),\n",
    "  (u'sportscenter', 'MD'),\n",
    "  (u'not', 'RB'),\n",
    "  (u'top', 'VB'),\n",
    "  (u'ten', 'NN'),\n",
    "  (u'plays', 'NNS'),\n",
    "  (u'pranks', 'NNS')],\n",
    " [(u'What', 'WP'),\n",
    "  (u'has', 'VBZ'),\n",
    "  (u'been', 'VBN'),\n",
    "  (u'the', 'DT'),\n",
    "  (u'most', 'RBS'),\n",
    "  (u'life-changing', 'JJ'),\n",
    "  (u'experience', 'NN'),\n",
    "  (u'in', 'IN'),\n",
    "  (u'your', 'PRP$'),\n",
    "  (u'life', 'NN'),\n",
    "  (u'?', '.')],\n",
    " [(u'On', 'IN'),\n",
    "  (u'repeat', 'NN'),\n",
    "  (u'for', 'IN'),\n",
    "  (u'most', 'JJS'),\n",
    "  (u'of', 'IN'),\n",
    "  (u'today', 'NN'),\n",
    "  (u'.', '.')]\n",
    "\n",
    "```\n",
    "\n",
    "After a long arduous tagging process plus consolidating all the tags used by nltk on my text dataset, I managed to get the following tags:\n",
    "```\n",
    "['PRP$', 'VBG', 'VBD', '``', 'VBN', 'POS', \"''\", 'VBP', 'WDT', 'JJ', 'WP', 'VBZ', 'DT', '#', 'RP', '$', 'NN', ')', '(', 'FW', ',', '.', 'TO', 'PRP', 'RB', ':', 'NNS', 'NNP', 'VB', 'WRB', 'CC', 'LS', 'PDT', 'RBS', 'RBR', 'CD', 'EX', 'IN', 'WP$', 'MD', 'NNPS', 'JJS', 'JJR', 'SYM', 'UH']\n",
    "```\n",
    "\n",
    "##### Mean or Median\n",
    "\n",
    "We ideally want to use the median value of a statistic as the median value is more resilient towards outliers than the mean. However in our case, as with the tags by nltk plus the meta-features as mentioned above, they do not appear very often which often results in returning a median of 0, which is a difficult number for us to use. For a cursory overview of the stats:\n",
    "\n",
    "```python\n",
    "for col in columnname:\n",
    "    newlist=[]\n",
    "    for line in mbti['tagged_words'][0]:\n",
    "        newlist.append(len([x for x in line if x[1]==col]))\n",
    "    print \"For \"+col+\",\"\n",
    "    print \"Sum = \", np.sum(newlist)\n",
    "    print \"Variance =\", np.var(newlist)\n",
    "    print \"Mean =\",np.mean(newlist)\n",
    "    print \"Median =\",np.median(newlist)\n",
    "    print \"Standard Deviation =\",np.std(newlist)\n",
    "```\n",
    "\n",
    "```\n",
    "Out:\n",
    "For PRP$,\n",
    "Sum =  16\n",
    "Variance = 0.3776\n",
    "Mean = 0.32\n",
    "Median = 0.0\n",
    "Standard Deviation = 0.614491659829\n",
    "For VBG,\n",
    "Sum =  14\n",
    "Variance = 0.3616\n",
    "Mean = 0.28\n",
    "Median = 0.0\n",
    "Standard Deviation = 0.601331855135\n",
    "For VBD,\n",
    "Sum =  11\n",
    "Variance = 0.4516\n",
    "Mean = 0.22\n",
    "Median = 0.0\n",
    "Standard Deviation = 0.672011904656\n",
    "For ``,\n",
    "Sum =  0\n",
    "Variance = 0.0\n",
    "Mean = 0.0\n",
    "Median = 0.0\n",
    "Standard Deviation = 0.0\n",
    "For VBN,\n",
    "Sum =  18\n",
    "Variance = 0.5104\n",
    "Mean = 0.36\n",
    "Median = 0.0\n",
    "Standard Deviation = 0.71442284398\n",
    "```\n",
    "\n",
    "On hindsight I could have put this into a dataframe but at this stage...\n",
    "\n",
    "For each tag, I took the mean and standard deviation as my column features.\n",
    "\n",
    "I still want to use the median as a determinant. How can I do so? Combine the tags!\n",
    "\n",
    "For interests sake, here is the full list of tags used by nltk, minus the punctuation tags:\n",
    "\n",
    "```\n",
    "#For reference ;)\n",
    "```\n",
    "Tag|Description\n",
    "---|------------\n",
    "CC| Coordinating conjunction\n",
    "CD| Cardinal number\n",
    "DT| Determiner\n",
    "EX| Existential there\n",
    "FW| Foreign word\n",
    "IN| Preposition or subordinating conjunction\n",
    "JJ| Adjective\n",
    "JJR| Adjective, comparative\n",
    "JJS| Adjective, superlative\n",
    "LS| List item marker\n",
    "MD| Modal\n",
    "NN| Noun, singular or mass\n",
    "NNS| Noun, plural\n",
    "NNP| Proper noun, singular\n",
    "NNPS| Proper noun, plural\n",
    "PDT| Predeterminer\n",
    "POS| Possessive ending\n",
    "PRP| Personal pronoun\n",
    "PRP\\$| Possessive pronoun\n",
    "RB| Adverb\n",
    "RBR| Adverb, comparative\n",
    "RBS| Adverb, superlative\n",
    "RP| Particle\n",
    "SYM| Symbol\n",
    "TO| to\n",
    "UH| Interjection\n",
    "VB| Verb, base form\n",
    "VBD| Verb, past tense\n",
    "VBG| Verb, gerund or present participle\n",
    "VBN| Verb, past participle\n",
    "VBP| Verb, non­3rd person singular present\n",
    "VBZ| Verb, 3rd person singular present\n",
    "WDT| Wh­determiner\n",
    "WP| Wh­pronoun\n",
    "WP\\$| Possessive wh­pronoun\n",
    "WRB| Wh­adverb\n",
    "\n",
    "\n",
    "We can already notice that there are multiple tags for each word type, each having a different function. This discrimination may be important for word generation to teach the machine how to write sentences properly, but once again, this is not my use case.\n",
    "\n",
    "Now compare that with POS tags by Stanford NLP:\n",
    "\n",
    "Tag|Meaning|English Examples\n",
    "---|--------|-----------\n",
    "ADJ|adjective|\tnew, good, high, special, big, local\n",
    "ADP|adposition|\ton, of, at, with, by, into, under\n",
    "ADV|adverb|\treally, already, still, early, now\n",
    "CONJ|conjunction|\tand, or, but, if, while, although\n",
    "DET|determiner, article|\tthe, a, some, most, every, no, which\n",
    "NOUN|noun|\tyear, home, costs, time, Africa\n",
    "NUM|numeral|\ttwenty-four, fourth, 1991, 14:24\n",
    "PRT|particle|\tat, on, out, over per, that, up, with\n",
    "PRON|pronoun|\the, their, her, its, my, I, us\n",
    "VERB|verb|\tis, say, told, given, playing, would\n",
    ".|\tpunctuation marks|\t. , ; !\n",
    "X|\tother|\tersatz, esprit, dunno, gr8, univeristy\n",
    "\n",
    "So much better!\n",
    "Group them together (based on my own understanding :laughing:)\n",
    "\n",
    "```python\n",
    "#Lets make a dictionary\n",
    "convtag_dict={'ADJ':['JJ','JJR','JJS'], 'ADP':['EX','TO'], 'ADV':['RB','RBR','RBS','WRB'], 'CONJ':['CC','IN'],'DET':['DT','PDT','WDT'],\n",
    "              'NOUN':['NN','NNS','NNP','NNPS'], 'NUM':['CD'],'PRT':['RP'],'PRON':['PRP','PRP$','WP','WP$'],\n",
    "              'VERB':['MD','VB','VBD','VBG','VBN','VBP','VBZ'],'.':['#','$',\"''\",'(',')',',','.',':'],'X':['FW','LS','UH']}\n",
    "```\n",
    "\n",
    "...And then get the median and standard deviation for each.\n",
    "\n",
    "Done.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Term Frequency - Inverse Document Frequency (TFIDF)\n",
    "\n",
    "This is the golden fine point where I start to deal with the dataset separately across the 4 MBTI types.\n",
    "\n",
    "**Rationale:** There is a methodology of how I am going to perform the TFIDF here.\n",
    "\n",
    "First of all, a one-liner summary about TFIDF: It is a measure for scoring words that appear often in a single document, but very rarely in other documents. Its use can be attributed to, once again for our use case, detecting the words that someone of an MBTI type would use more often collectively as opposed to another.\n",
    "\n",
    "For each run of the target variable (I'll start with Introversion/Extraversion), a train test split will be done using stratified sampling, especially important for E/I and S/N target variables. Since each row (or data point) identifies differently with each variable, the collection of rows belonging to X_train in E/I will definitely be different from the X_train of S/N. This is important because I will only use the X_train portion to train the TFIDF model, followed by having the model transform the X_test word data. This measure ensures that the word data in the test set has no role in the dataset training which also means avoiding overrepresentation or overfitting.\n",
    "\n",
    "And so this is where we start to part ways.\n",
    "\n",
    "For training purposes we can specify the removal of stop words (Common words such as \"a, an, the\" etc), a word range (otherwise known as ngram range) and the max number of words/phrases, among other features not as worth highlighting here. Last I ran with an ngram range of 1-3 and no specified max. features, I got around 300,000+ rows in return. It is a miracle my computer did not crash.\n",
    "\n",
    "To better manage the TFIDF word results while extracting good features at the same time, I would:\n",
    "1. Run the TFIDF model with ngram range of 1, and max. features of 10,000 words/phrases\n",
    "2. Reduce number of features to 500 using Truncated Singular Value Decomposition (Truncated SVD)\n",
    "3. Save both TFIDF and TSVD instances into a list for later use.\n",
    "\n",
    "For reference, below is the code that runs the mentioned process:\n",
    "```python\n",
    "for i in np.arange(1,4):\n",
    "    tfidf = TfidfVectorizer(stop_words='english',ngram_range=(i,i), decode_error='replace', max_features=10000)\n",
    "    Xword_train = tfidf.fit_transform(X_train['words_only'])\n",
    "    Xword_test = tfidf.transform(X_test['words_only'])\n",
    "\n",
    "    #We need to reduce the size of the tfidf trained matrix first\n",
    "    #But after running TruncatedSVD we cannot see the words specifically alr so too bad...\n",
    "    tsvd = TruncatedSVD(n_components=500, algorithm='arpack', random_state=self.random_state)\n",
    "    Xwordie_train = tsvd.fit_transform(Xword_train)\n",
    "    Xwordie_test = tsvd.transform(Xword_test)\n",
    "    Xwordie_train_df = pd.DataFrame(Xwordie_train,\n",
    "                                    columns=[str(i)+'_'+str(b) for b in np.arange(1,Xwordie_train.shape[1]+1)])\n",
    "    Xwordie_test_df = pd.DataFrame(Xwordie_test,\n",
    "                                   columns=[str(i)+'_'+str(b) for b in np.arange(1,Xwordie_test.shape[1]+1)])\n",
    "    df_train = pd.concat([df_train,Xwordie_train_df], axis=1)\n",
    "    df_test = pd.concat([df_test,Xwordie_test_df], axis=1)\n",
    "    self.tfidf_list.append(tfidf)\n",
    "    self.tsvd_list.append(tsvd)\n",
    "```\n",
    "\n",
    "Oh yes, I should explain TSVD.\n",
    "\n",
    "TSVD, from my own limited understanding, is a reduction method much like Principal Component Analysis (PCA), except that it only 'shrinks' vertically. It is commonly used together with TFIDF since TSVD has the ability to 'merge' together word vectors that have similar scores in the dataset (in simple stats language, high positive correlation). Such modelling would tend to group together words that belong to similar topics, since they appear in large amounts in a small subset of documents.\n",
    "\n",
    "Ok I admit, I just plug and played :X\n",
    "\n",
    "So in my case, the end result would be 1500 truncated columns of various ngram range. This method also helps manage computer memory :wink:\n",
    "\n",
    "Unfortunately for me I kinda 'lost' the words used in the process so...\n",
    "\n",
    "### Feature removal\n",
    "\n",
    "Sad to say, there are some features that I just have to remove. For the purpose of my use case (classify text data), it would be unrealistic for such features to be used. Not like we talk about MBTI all the time, do we?\n",
    "\n",
    "```python\n",
    "X_train.drop(['n_video','n_links','n_image','n_otherlink','mention_count','hashtag_count','mbti_ref_count','ennea_count',\n",
    "                          'bracket_count'], axis=1, inplace=True)\n",
    "```\n",
    "\n",
    "### Scaling\n",
    "\n",
    "I did two types of scaling: Standard Scaling and MinMax Scaling.\n",
    "\n",
    "Standard Scaling works to 'level' the field across column features. Quite commonly in machine learning execution, we get imbalanced representation where one feature would have a range of 100000 while another probably only has a range of 0.001 (and therefore, the former would significantly affect the scoring function more). What Standard Scaling does is simply subtract each value from the mean and then divide by the standard deviation. Naturally, features with large ranges gets 'penalized' more heavily.\n",
    "\n",
    "I forgot to plot using my own data, so hopefully this imagery from a class project would help :sweat_smile:\n",
    "\n",
    "Before:\n",
    "![ss_before](ss_before.png)\n",
    "After:\n",
    "![ss_after](ss_after.png)\n",
    "\n",
    "I can't scale this pesky shit :angry:\n",
    "\n",
    "So...I only applied this on the other metadata that I ported over.\n",
    "\n",
    "MinMax Scaling was done primarily for the next feature selection technique that I used: Chi square. Chi square selection only works with positive values, so I kinda have to 'scale up' the negative values up. What this scaling essentially does is to scale the data to a specified range, namely between 1 and 0\n",
    "\n",
    "(On hindsight I probably didn't need the standard scaling but heck)\n",
    "\n",
    "### Under-Sampling the majority class\n",
    "\n",
    "We still do have imbalanced datasets (for E/I and S/N) which necessitates the use of stratified sampling during the train test split. An imbalanced dataset is generally bad for machine learning as it will tend to predict more readily the majority class. One way to provide the balance is to either under-sample the majority class or to over-sample the minority class. I chose to do the former only. Somehow.\n",
    "\n",
    "```python\n",
    "if imbl:\n",
    "    imbler = RandomUnderSampler(random_state=42)\n",
    "    X_train, y_train = imbler.fit_sample(X_train, y_train)\n",
    "```\n",
    "\n",
    "### More Feature reduction!\n",
    "\n",
    "Use Chi square. Once again, I can't explain it :pensive: except that I reduced it further to 100 features.\n",
    "\n",
    "Anyway! Here are the results in the case of Introversion/Extraversion, top ten!:\n",
    "\n",
    "|Features|\tScores|\tp-value\n",
    "-|-------|--------|-------\n",
    "37|\t1_15|\t5.958086|\t0.014650\n",
    "33|\t1_11|\t5.582567|\t0.018140\n",
    "3|\tn_caps_char|\t4.601679|\t0.031941\n",
    "2|\tn_caps|\t3.875362|\t0.049000\n",
    "30|\t1_7|\t3.746265|\t0.052926\n",
    "38|\t1_16|\t3.508324|\t0.061061\n",
    "28|\t1_3|\t3.445238|\t0.063434\n",
    "31|\t1_9|\t3.156438|\t0.075628\n",
    "32|\t1_10|\t2.853656|\t0.091166\n",
    "27|\t1_2|\t2.413678|\t0.120279\n",
    "\n",
    "### At long last, modelling!\n",
    "\n",
    "Using the good old Logistic Regression with lasso penalty,\n",
    "\n",
    "**Result:**\n",
    "\n",
    "__Introversion/Extraversion__\n",
    "\n",
    "Score: 0.820749279539\n",
    "\n",
    "Cross val score: [ 0.8261563   0.80718954  0.81730769  0.82670906  0.80830671]\n",
    "\n",
    "type|precision|recall|f1-score|support\n",
    "----------|----------|------------|--------|----------\n",
    "  Introvert|       0.93|      0.83|      0.88 |     1335\n",
    "  Extrovert|       0.58|      0.78|      0.67 |     400\n",
    "avg / total|       0.85|      0.82|      0.83 |     1735\n",
    "\n",
    "\n",
    "| Introvert_pred|  Extrovert_pred\n",
    "--------|--------------|---------------    \n",
    "Introvert_true|            1113|              222\n",
    "Extrovert_true|              89|              311\n",
    "\n",
    "Sorry I didn't really keep records of other models, but this works!\n",
    "\n",
    "We shall look at other 3 types:\n",
    "\n",
    "__Sensing/Intuition__\n",
    "\n",
    "Score: 0.821902017291\n",
    "\n",
    "Cross val score: [ 0.83544304  0.82446809  0.83028721  0.83018868  0.83684211]\n",
    "\n",
    "type|precision|    recall|  f1-score|   support\n",
    "----|----------|----------|--------|-----------\n",
    "  Intuitive|       0.97|      0.82|      0.89|      1496\n",
    "    Sensing|       0.42|      0.82|      0.56|       239\n",
    "avg / total|       0.89|      0.82|      0.84|      1735\n",
    "\n",
    "| Intuitive_pred|  Sensing_pred\n",
    "--|--------------|----------------\n",
    "Intuitive_true|            1231|            265\n",
    "Sensing_true|                44|            195\n",
    "\n",
    "__Thinking/Feeling__\n",
    "\n",
    "Score: 0.841498559078\n",
    "\n",
    "Cross val score: [ 0.82605364  0.8696331   0.84748428  0.83333333  0.84507042]\n",
    "\n",
    "type|precision|    recall|  f1-score|   support\n",
    "----|----------|--------|---------|---------\n",
    "    Feeling|       0.85|      0.86|      0.85|       939\n",
    "   Thinking|       0.83|      0.83|      0.83|       796\n",
    "avg / total|       0.84|      0.84|      0.84|      1735\n",
    "\n",
    "|Feeling_pred|  Thinking_pred\n",
    "-|-------------|---------\n",
    "Feeling_true|            803|136\n",
    "Thinking_true|           139|657\n",
    "\n",
    "\n",
    "__Judging/Perceiving__\n",
    "\n",
    "Score: 0.796541786744\n",
    "\n",
    "Cross val score: [ 0.79491833  0.79855465  0.79597438  0.78405931  0.79851439]\n",
    "\n",
    "type| precision|    recall|  f1-score|   support\n",
    "----|---------|---------|---------|-----------\n",
    " Perceiving|       0.84|      0.82|      0.83|      1048\n",
    "    Judging|       0.73|      0.77|      0.75|       687\n",
    "avg / total|       0.80|      0.80|      0.80|      1735\n",
    "\n",
    "| Perceiving_pred|  Judging_pred\n",
    "-|---------------|-------------|\n",
    "Perceiving_true|              856|            192\n",
    "Judging_true|                 161|            526\n",
    "\n",
    "\n",
    "## The TPOT fallacy\n",
    "\n",
    "When, in your data science work, you reached the modelling stage where you need to figure out the best model to use, and the process becomes iterative...\n",
    "\n",
    "TPOT to the rescue! Given a few random parameters, it will iterate for you all sorts of models with different hyperparameters and even some scaling to give you the best possible model.\n",
    "\n",
    "```python\n",
    "from tpot import TPOTClassifier\n",
    "tpot = TPOTClassifier(generations=10, population_size=30, verbosity=2, scoring='f1')\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_test, y_test))\n",
    "tpot.export('tpot_E_try.py')\n",
    "```\n",
    "```\n",
    "Generation 1 - Current best internal CV score: 0.817470446061\n",
    "                                                                               \n",
    "Generation 2 - Current best internal CV score: 0.817470446061\n",
    "                                                                               \n",
    "Generation 3 - Current best internal CV score: 0.817470446061\n",
    "                                                                               \n",
    "Generation 4 - Current best internal CV score: 0.817470446061\n",
    "                                                                               \n",
    "Generation 5 - Current best internal CV score: 0.818117900031\n",
    "                                                                               \n",
    "Generation 6 - Current best internal CV score: 0.818117900031\n",
    "                                                                               \n",
    "Generation 7 - Current best internal CV score: 0.818117900031\n",
    "                                                                               \n",
    "Generation 8 - Current best internal CV score: 0.819767096337\n",
    "                                                                               \n",
    "Generation 9 - Current best internal CV score: 0.819767096337\n",
    "                                                                               \n",
    "Generation 10 - Current best internal CV score: 0.819767096337\n",
    "                                                             \n",
    "\n",
    "Best pipeline: LogisticRegression(FastICA(ZeroCount(MaxAbsScaler(input_matrix)), tol=0.1), C=20.0, dual=False, penalty=l1)\n",
    "```\n",
    "\n",
    "OR SO I THOUGHT. AGAIN.\n",
    "\n",
    "Score: 0.665254237288 :expressionless:\n",
    "\n",
    "It happened similarly with other TPOT runs for other 3 MBTI types.\n",
    "\n",
    "Look at all the cheem modelling, and they still cannot beat the simple model!\n",
    "\n",
    "Important life lesson as a data scientist: Cheem and fancy isn't everything!\n",
    "\n",
    "## Whats next\n",
    "\n",
    "I wrote another class that would train incoming inputs for use in training (for application!)\n",
    "\n",
    "Basically I had to condense all the processing code into a class for preprocessing, followed by 'borrowing' loads of model instances trained within the instances of the 4 MBTI types.\n",
    "\n",
    "```python\n",
    "#Create an instance\n",
    "Someguy = NewBerd()\n",
    "\n",
    "#Perform preprocessing for each line of text\n",
    "for line in mbti_textlist:\n",
    "    Someguy.preprocess(line, web=False)\n",
    "    \n",
    "#Predict!\n",
    "more_magic(Someguy)\n",
    "```\n",
    "\n",
    "Now that you made it to this point, I hope you have not forgotten about the webapp hahah.\n",
    "\n",
    "[Click here!](https://yix90.github.io)\n",
    "\n",
    "## Future work\n",
    "\n",
    "I have to admit, I am abit lucky in a sense that the modeling turned out not too badly. There is still much to be done, both for this project and my own data science journey:\n",
    "\n",
    "* Be able to understand the models that I have utilized so far\n",
    "* Perform some more feature extraction from images and webpages, followed by topic modelling (I haven't forgotten!)\n",
    "* Try other feature reduction techniques\n",
    "* Try using other predicting models\n",
    "* Try my hand at deep learning\n",
    "\n",
    "On top of doing the above, I also hope to explore the following:\n",
    "\n",
    "* Perform an Enneagram classifier\n",
    "* Combine the MBTI and Enneagram classifier models and see how each MBTI type relates to each Enneagram type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
