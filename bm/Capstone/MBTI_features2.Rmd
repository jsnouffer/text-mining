---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.1'
      jupytext_version: 1.2.3
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

## Feature extraction part 2!

```{python}
import pandas as pd
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
import re
import scipy.stats as stats
import datetime as dt


sns.set_style('darkgrid')
# %config InlineBackend.figure_format = 'retina'
# %matplotlib inline
```

```{python}
mbti = pd.read_pickle('mbti_ver1.pickle')
```

```{python}
mbti.shape
```

```{python}
#Get median for number of characters/words per post in each row

mbti['med_char'] = mbti['words_only'].apply(lambda x: np.median([len(i) for i in x]))
mbti['med_word'] = mbti['words_only'].apply(lambda x: np.median([len(i.split()) for i in x]))
mbti.shape
mbti.head()
```

```{python}
#Spice things up a little by splitting to individual types
mbti['is_E'] = mbti['type'].apply(lambda x: 1 if x[0] == 'E' else 0)
mbti['is_S'] = mbti['type'].apply(lambda x: 1 if x[1] == 'S' else 0)
mbti['is_T'] = mbti['type'].apply(lambda x: 1 if x[2] == 'T' else 0)
mbti['is_J'] = mbti['type'].apply(lambda x: 1 if x[3] == 'J' else 0)

#Yes this is the direction I am gonna go.
```

```{python}
mbti['word_char_ratio'] = mbti['med_char'] / mbti['med_word']
```

```{python}
mbti.columns
```

```{python}
#Look at number by descending count per type
mbti[['type','emoticon_count']].sort_values('emoticon_count', ascending=False).head(10)
#But then again this can be skewed towards higher participation, so...
```

```{python}
#We look at averages below
#By eyeballing we can see that feelers tend to use more emoticons on average than thinkers with the exception of ISTJ
#The rest are more uhh uniform
mbti.groupby('type')['emoticon_count'].apply(np.mean).sort_values(ascending=False)
```

```{python}
#If we look at 'is_T' instead (1 = T, 0 = F)
plt.figure(figsize=(6,6))
plt.subplot(221)
E = mbti.groupby('is_E')['emoticon_count'].apply(np.mean).sort_values(ascending=False)
plt.bar(E.index, E, width=0.3, tick_label=['I','E'], color='yellow')
plt.ylabel("n_emoticon")
plt.title("Average emoticon count of Extraverted (E) vs Introverted (I) types")

plt.subplot(222)
S = mbti.groupby('is_S')['emoticon_count'].apply(np.mean).sort_values(ascending=False)
plt.bar(S.index, S, width=0.3, tick_label=['N','S'], color='red')
plt.ylabel("n_emoticon")
plt.title("Average emoticon count of Sensing (S) vs Intiutive (N) types")

plt.subplot(223)
T = mbti.groupby('is_T')['emoticon_count'].apply(np.mean).sort_values(ascending=False)
plt.bar(T.index, T, width=0.3, tick_label=['F','T'])
plt.ylabel("n_emoticon")
plt.title("Average emoticon count of Thinking (T) vs Feeling (F) types")

plt.subplot(224)
J = mbti.groupby('is_J')['emoticon_count'].apply(np.mean).sort_values(ascending=False)
plt.bar(J.index, J, width=0.3, tick_label=['P','J'], color='green')
plt.ylabel("n_emoticon")
plt.title("Average emoticon count of Judging (J) vs Perceptive (P) types")

plt.subplots_adjust(left=0, bottom=1, right=1.1, top=2, wspace=2, hspace=0.3)
```

```{python}
def compareplot(what):
    plt.figure(figsize=(6,6))
    plt.subplot(221)
    E = mbti.groupby('is_E')[what].apply(np.mean).sort_values(ascending=False)
    plt.bar(E.index, E, width=0.3, tick_label=['I','E'], color='yellow')
    plt.ylabel(what)
    plt.title("Average "+what+" of Extraverted (E) vs Introverted (I) types")

    plt.subplot(222)
    S = mbti.groupby('is_S')[what].apply(np.mean).sort_values(ascending=False)
    plt.bar(S.index, S, width=0.3, tick_label=['N','S'], color='red')
    plt.ylabel(what)
    plt.title("Average "+what+" of Sensing (S) vs Intiutive (N) types")

    plt.subplot(223)
    T = mbti.groupby('is_T')[what].apply(np.mean).sort_values(ascending=False)
    plt.bar(T.index, T, width=0.3, tick_label=['F','T'])
    plt.ylabel(what)
    plt.title("Average "+what+" of Thinking (T) vs Feeling (F) types")

    plt.subplot(224)
    J = mbti.groupby('is_J')[what].apply(np.mean).sort_values(ascending=False)
    plt.bar(J.index, J, width=0.3, tick_label=['P','J'], color='green')
    plt.ylabel(what)
    plt.title("Average "+what+" of Judging (J) vs Perceptive (P) types")

    plt.subplots_adjust(left=0, bottom=1, right=1.1, top=2, wspace=2, hspace=0.3)
```

```{python}
mbti.columns
```

```{python}
compareplot('word_char_ratio')
```

```{python}
colli = ['n_video','n_links','n_image','emoticon_count','mention_count','hashtag_count','mbti_ref_count',
       'ennea_count', 'bracket_count', 'dots_count', 'n_char', 'n_word','n_action', 'n_caps','n_caps_char',
       'word_cap_ratio','char_cap_ratio','med_char','med_word','is_E','is_S','is_T','is_J']
mbti_num = mbti[colli]
mbti_num.head()
```

```{python}
from statsmodels.formula.api import logit
logs = logit(formula='is_E ~ '+' + '.join(mbti_num.drop(['is_E','is_S','is_T','is_J'], axis=1).columns), data=mbti_num).fit()
logs.summary()
```

```{python}
formu = 'is_E ~ n_video + emoticon_count + mention_count + mbti_ref_count + ennea_count + n_char + n_word + n_action + n_caps + word_cap_ratio + char_cap_ratio - 1'
logs = logit(formula=formu, data=mbti_num).fit()
logs.summary()
```

```{python}
' + '.join(mbti_num.drop(['is_E','is_S','is_T','is_J'], axis=1).columns)
```

```{python}
sns.pairplot(mbti_num[['is_E','emoticon_count','mention_count','hashtag_count','mbti_ref_count',
       'ennea_count', 'bracket_count', 'dots_count']], hue='is_E')
```

```{python}
sns.pairplot(mbti_num[['is_E','n_char', 'n_word','n_action', 'n_caps','n_caps_char',
       'word_cap_ratio','char_cap_ratio','med_char','med_word']], hue='is_E')
```

```{python}
plt.figure(figsize=(8,8))
sns.heatmap(mbti_num[['is_E','n_char', 'n_word','n_action', 'n_caps','n_caps_char',
       'word_cap_ratio','char_cap_ratio','med_char','med_word']].corr(), annot=True)
```

```{python}
y = mbti_num['is_E']
X = mbti_num[['n_video','n_links','n_image','emoticon_count','mention_count','hashtag_count','mbti_ref_count',
       'ennea_count', 'bracket_count', 'dots_count', 'n_char', 'n_word','n_action', 'n_caps','n_caps_char',
       'word_cap_ratio','char_cap_ratio','med_char','med_word']]
```

I would like to first try out TPOT here

See how the performance will be like

```{python}
from tpot import TPOTClassifier
from sklearn.model_selection import train_test_split

y = mbti_num['is_E']
X = mbti_num[['n_video','n_links','n_image','emoticon_count','mention_count','hashtag_count','mbti_ref_count',
       'ennea_count', 'bracket_count', 'dots_count', 'n_char', 'n_word','n_action', 'n_caps','n_caps_char',
       'word_cap_ratio','char_cap_ratio','med_char','med_word']]
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42, stratify=y)

tpot = TPOTClassifier(generations=5, population_size=20, verbosity=2, scoring='roc_auc')
tpot.fit(X_train, y_train)
print(tpot.score(X_test, y_test))
tpot.export('tpot_E_try.py')
```

```{python jupyter={'outputs_hidden': True}}
# y = mbti_num['is_S']
# X = mbti_num[['n_video','n_links','n_image','emoticon_count','mention_count','hashtag_count','mbti_ref_count',
#        'ennea_count', 'bracket_count', 'dots_count', 'n_char', 'n_word','n_action', 'n_caps','n_caps_char',
#        'word_cap_ratio','char_cap_ratio','med_char','med_word']]
# X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42, stratify=y)

# tpot = TPOTClassifier(generations=10, population_size=30, verbosity=2, scoring='roc_auc')
# tpot.fit(X_train, y_train)
# print(tpot.score(X_test, y_test))
# tpot.export('tpot_S_try.py')
```

```{python jupyter={'outputs_hidden': True}}
# y = mbti_num['is_T']
# X = mbti_num[['n_video','n_links','n_image','emoticon_count','mention_count','hashtag_count','mbti_ref_count',
#        'ennea_count', 'bracket_count', 'dots_count', 'n_char', 'n_word','n_action', 'n_caps','n_caps_char',
#        'word_cap_ratio','char_cap_ratio','med_char','med_word']]
# X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42, stratify=y)

# tpot = TPOTClassifier(generations=10, population_size=30, verbosity=2, scoring='roc_auc')
# tpot.fit(X_train, y_train)
# print(tpot.score(X_test, y_test))
# tpot.export('tpot_T_try.py')
```

```{python jupyter={'outputs_hidden': True}}
# y = mbti_num['is_J']
# X = mbti_num[['n_video','n_links','n_image','emoticon_count','mention_count','hashtag_count','mbti_ref_count',
#        'ennea_count', 'bracket_count', 'dots_count', 'n_char', 'n_word','n_action', 'n_caps','n_caps_char',
#        'word_cap_ratio','char_cap_ratio','med_char','med_word']]
# X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42, stratify=y)

# tpot = TPOTClassifier(generations=10, population_size=30, verbosity=2, scoring='roc_auc')
# tpot.fit(X_train, y_train)
# print(tpot.score(X_test, y_test))
# tpot.export('tpot_J_try.py')
```

### Part's of speech tagging

```{python}
import nltk
from nltk.tokenize import word_tokenize
import swifter

# nltk.download('punkt')
# nltk.download('averaged_perceptron_tagger')

#Takes a long time to run!
mbti['tagged_words'] = mbti['words_only'].swifter.apply(
    lambda x: [nltk.pos_tag(word_tokenize(line)) for line in x])
#mbti['tagged_words'][0]
```

```{python}
mbti.to_pickle('../after_tagged_words')
```

```{python}
#From the 1st X rows, look at the type of words in each tag
#Also takes a long time! I used a for loop -.-

from pprint import pprint
wordcatdict={}
for pig in mbti['tagged_words']:
    for line in pig:
        for x in line:
            if x[1] in wordcatdict.keys():         #Check for existing tag
                if x[0] in wordcatdict[x[1]]:      #Check for existing word
                    continue
                else:
                    wordcatdict[x[1]].append(x[0]) #Add 2nd or more word
            else:
                wordcatdict[x[1]] = [x[0]]         #Add new tag with new word as list

```

```{python}
len(wordcatdict.keys())
```

```{python}
columnname = [col for col in wordcatdict.keys()]
print(columnname)
```

```{python}
for col in columnname:
    newlist=[]
    for line in mbti['tagged_words'][0]:
        newlist.append(len([x for x in line if x[1]==col]))
    print "For "+col+","
    print "Sum = ", np.sum(newlist)
    print "Variance =", np.var(newlist)
    print "Mean =",np.mean(newlist)
    print "Median =",np.median(newlist)
    print "Standard Deviation =",np.std(newlist)
```

So...

Variance and median looks useless... since some words are not used very often. We are dealing with posts of varying lengths and they may tend to contain a wide range, from little/no words to a lengthy discussion.

If I run my model with a chatbot it might or might not succeed, since I reckon that text messaging would tend to be shorter.

Some adjustments need to be made

```{python}
#Let us visualise one of the word types across a spectrum
def plot_wordtype(ind, wtype):
    newlist=[]
    for line in mbti['tagged_words'][ind]:
        newlist.append(len([x for x in line if x[1]==wtype]))
        
    newlist.sort()
    plt.figure(figsize=(10,5))
    plt.plot(np.arange(1, len(newlist)+1), newlist)
    plt.xlabel("Index")
    plt.ylabel("Frequency of "+wtype+" word")
    plt.title("Frequency plot of words of type "+wtype+" for MBTI type "+mbti['type'][ind])
```

```{python}
plot_wordtype(145, 'RB')
```

In addition to simply counting all that occurs, I think it would be good to kinda 'summarize' them

Like for example the nouns come in many forms, like NN, NNP, NNS, NNPS - but we can group all together, for instance.

Such distinction is useful for word generation I guess but not that useful for purpose of my use.

Lets see... Now I shall do the following:

Tag	| Meaning	|English Examples
----|-----------|------------------
ADJ	|adjective	|new, good, high, special, big, local
ADP	|adposition	|on, of, at, with, by, into, under
ADV	|adverb	|really, already, still, early, now
CONJ |conjunction	|and, or, but, if, while, although
DET	|determiner, article	|the, a, some, most, every, no, which
NOUN|	noun	|year, home, costs, time, Africa
NUM	|numeral	|twenty-four, fourth, 1991, 14:24
PRT	|particle	|at, on, out, over per, that, up, with
PRON|	pronoun	|he, their, her, its, my, I, us
VERB|	verb	|is, say, told, given, playing, would
.	|punctuation marks	|. , ; !
X	|other	|ersatz, esprit, dunno, gr8, univeristy

But I will still keep the original columns heh

```{python}
#Create columns to get the mean and std for each POS tagging for each row
def pos_cat(x, wordie):
    return [len([y for y in line if y[1] == wordie])for line in x]
    

for col in columnname:
    mbti['POS_'+col+'_mean'] = mbti['tagged_words'].apply(lambda x: np.mean(pos_cat(x, col)))
    mbti['POS_'+col+'_std'] = mbti['tagged_words'].apply(lambda x: np.std(pos_cat(x, col)))
mbti.head()
```

```{python}
mbti.to_pickle('../after_pos_stats')
```

```{python}
#For reference ;)
lollie = """
CC Coordinating conjunction *
CD Cardinal number *
DT Determiner *
EX Existential there *
FW Foreign word *
IN Preposition or subordinating conjunction *
JJ Adjective *
JJR Adjective, comparative *
JJS Adjective, superlative *
LS List item marker *
MD Modal *
NN Noun, singular or mass *
NNS Noun, plural *
NNP Proper noun, singular *
NNPS Proper noun, plural *
PDT Predeterminer *
POS Possessive ending ?
PRP Personal pronoun *
PRP$ Possessive pronoun *
RB Adverb *
RBR Adverb, comparative *
RBS Adverb, superlative *
RP Particle *
SYM Symbol ?
TO to *
UH Interjection *
VB Verb, base form *
VBD Verb, past tense
VBG Verb, gerund or present participle *
VBN Verb, past participle *
VBP Verb, non­3rd person singular present *
VBZ Verb, 3rd person singular present *
WDT Wh­determiner *
WP Wh­pronoun *
WP$ Possessive wh­pronoun *
WRB Wh­adverb *
"""

#Lets make a dictionary
convtag_dict={'ADJ':['JJ','JJR','JJS'], 'ADP':['EX','TO'], 'ADV':['RB','RBR','RBS','WRB'], 'CONJ':['CC','IN'],'DET':['DT','PDT','WDT'],
              'NOUN':['NN','NNS','NNP','NNPS'], 'NUM':['CD'],'PRT':['RP'],'PRON':['PRP','PRP$','WP','WP$'],
              'VERB':['MD','VB','VBD','VBG','VBN','VBP','VBZ'],'.':['#','$',"''",'(',')',',','.',':'],'X':['FW','LS','UH']}
```

```{python}
def bigcol_tag(x, col):
    newlist = [len([y for y in line if y[1] in convtag_dict[col]])for line in x]       
    return newlist

for col in convtag_dict.keys():  
    mbti['BIGPOS_'+col+'_med'] = mbti['tagged_words'].apply(lambda x: np.median(bigcol_tag(x, col)))
    mbti['BIGPOS_'+col+'_std']= mbti['tagged_words'].apply(lambda x: np.std(bigcol_tag(x, col)))
```

```{python jupyter={'outputs_hidden': True}}
# mbti['n_mbti_self'] = mbti.apply(lambda x: 1 if x['type']==x['mbti_ref_most'] else 0, axis=1)
```

Pickle

```{python}
mbti.to_pickle('mbti_ver2.pickle')
```

# \**Checkpoint \**

```{python jupyter={'outputs_hidden': True}}
mbti = pd.read_pickle('mbti_ver2.pickle')
```

```{python}
#Take out irrelevant columns
mbti_w = mbti[['words_only','image','video_title','otherlink','emoticons','ennea','tagged_words']]
mbti_n = mbti.drop(['type','image','video_link','video_title',
                    'otherlink','emoticons','hashtag','mbti_ref','mbti_ref_most','ennea','tagged_words',
                   'tagged_words', 'is_E','is_S','is_T','is_J'], axis=1)
```

```{python}
mbti_n['words_only'] = mbti_n['words_only'].apply(lambda x: ' '.join(x))
```

# Term Frequency - Inverse Document Frequency

From here on I will start to split the dataset by the following:

1. Duplicate the set by 4, with 4 types of ys using is_E, is_S, is_T and is_J
2. Fit the model once with the full set
3. Do a stratified split for all 4 sets, so I will have 4 sets of X_train and 4 sets of X_test
4. Transform all of them
5. Using a chi-squared test, try to scale down the dimension separately

So from here on actually we might start to see how each feature may be more effective in one model than another. From here on each model may start to 'deviate' from one another



```{python}
#We love sklearn <3 <3
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.feature_selection import chi2, SelectKBest
from sklearn.decomposition import TruncatedSVD
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.linear_model import LogisticRegression
from imblearn.under_sampling import RandomUnderSampler
import nltk
from nltk.tokenize import word_tokenize
```

```{python}
mbti_n['word_char_ratio'] = mbti_n['word_char_ratio'].fillna(np.median(mbti_n[~mbti_n['word_char_ratio'].isnull()]['word_char_ratio']))
```

```{python}
y_E = mbti['is_E']
y_S = mbti['is_S']
y_T = mbti['is_T']
y_J = mbti['is_J']
X = mbti_n

class Thipe(object):
    
    def __init__(self, X, y, stan=True, rand=42, web=True, include_feature='all'):
        self.stan=stan
        self.web=web
        self.include_feature=include_feature
        self.X=X
        self.y=y
        self.random_state = rand
        self.columns=None
        self.X_train=None
        self.y_train=None
        self.columnie=[]
        self.columners=[]
        #I will be porting some features to other classes sooooo...
        self.tfidf_list=[]
        self.tsvd_list=[]
        self.ss=None
        self.mms=None
        self.ch2=None
        self.sexy=None
        
    def baseline_acc(self):
        baseline = max(self.y.value_counts()[0], self.y.value_counts()[1]) / float(self.y.value_counts().sum())
        return baseline * 100,'\%'
        
        
    def trainy(self, testy=0.2, imbl=True):
        """
        I'll do the following here:
        1. Do train test split
        2. Convert X_train and X_test to DataFrame (to delete column later plus other purposes)
        3. Do tfidf using train section, use the model and fit the X_train and X_test (then can delete the wordchunk column)
        4. If StandardScaler, scale the training and test data. (Default = True)
        5. To prepare data for chi2 reduction we need to scale everything to above 0, so MinMaxScaler
        """
               
        #This is perhaps the main reason why this step is embedded in a class
        #Because the stratification would be different, everything would be different already, like the tfidf vocab for example
        X_train, X_test, y_train, y_test = train_test_split(self.X, self.y, random_state=self.random_state,
                                                                      test_size=testy, stratify=self.y)
            
        self.y_train=y_train
        self.y_test=y_test     
        X_train = pd.DataFrame(X_train, columns=self.columns)
        X_test = pd.DataFrame(X_test, columns=self.columns)
        df_train = pd.DataFrame()
        df_test = pd.DataFrame()
        
        for i in np.arange(1,4):
            tfidf = TfidfVectorizer(stop_words='english',ngram_range=(i,i), decode_error='replace', max_features=10000)
            Xword_train = tfidf.fit_transform(X_train['words_only'])
            Xword_test = tfidf.transform(X_test['words_only'])

            #We need to reduce the size of the tfidf trained matrix first
            #But after running TruncatedSVD we cannot see the words specifically alr so too bad...
            tsvd = TruncatedSVD(n_components=500, algorithm='arpack', random_state=self.random_state)
            Xwordie_train = tsvd.fit_transform(Xword_train)
            Xwordie_test = tsvd.transform(Xword_test)
            Xwordie_train_df = pd.DataFrame(Xwordie_train,
                                            columns=[str(i)+'_'+str(b) for b in np.arange(1,Xwordie_train.shape[1]+1)])
            Xwordie_test_df = pd.DataFrame(Xwordie_test,
                                           columns=[str(i)+'_'+str(b) for b in np.arange(1,Xwordie_test.shape[1]+1)])
            df_train = pd.concat([df_train,Xwordie_train_df], axis=1)
            df_test = pd.concat([df_test,Xwordie_test_df], axis=1)
            self.tfidf_list.append(tfidf)
            self.tsvd_list.append(tsvd)        
        
        X_train.drop(['words_only'],axis=1,inplace=True)
        X_test.drop(['words_only'],axis=1,inplace=True)
        X = self.X.drop(['words_only'], axis=1)
        if self.web:
            X_train.drop(['n_video','n_links','n_image','n_otherlink','mention_count','hashtag_count','mbti_ref_count','ennea_count',
                          'bracket_count'], axis=1, inplace=True)
            X_test.drop(['n_video','n_links','n_image','n_otherlink','mention_count','hashtag_count','mbti_ref_count','ennea_count',
                          'bracket_count'], axis=1, inplace=True)
            X.drop(['n_video','n_links','n_image','n_otherlink','mention_count','hashtag_count','mbti_ref_count','ennea_count',
                          'bracket_count'], axis=1, inplace=True)
        self.columns = X_train.columns
        
        #Standardization step
        if self.stan:
            ss = StandardScaler().fit(X)
            X_train = ss.transform(X_train)
            X_test = ss.transform(X_test)
            X_train = pd.DataFrame(X_train, columns=self.columns)
            X_test = pd.DataFrame(X_test, columns=self.columns)
            self.ss = ss
            
        #Join step
        if self.include_feature == 'words':
            X_train = df_train
            X_test = df_test
            self.columnie = X_train.columns
        else:
            X_train = X_train.join(df_train)
            X_test = X_test.join(df_test)
            self.columnie = X_train.columns
            
        
        #Scale again to between 0 and 1
        combined_X = pd.concat([X_train,X_test],axis=0)
        mms = MinMaxScaler().fit(combined_X)
        X_train = pd.DataFrame(mms.transform(X_train), columns=self.columnie)
        X_test = pd.DataFrame(mms.transform(X_test), columns=self.columnie)
               
        if imbl:
            imbler = RandomUnderSampler(random_state=42)
            X_train, y_train = imbler.fit_sample(X_train, y_train)
        
        self.X_train = X_train
        self.X_test = X_test
        self.y_train = y_train
        self.mms = mms
        
        return X_train, X_test, y_train, y_test
    
    def reducy(self, n_features=100):
        """
        Use chi2 to shrink down features
        """
        ch2 = SelectKBest(chi2, k=n_features)
        X_train = ch2.fit_transform(self.X_train, self.y_train)
        X_test = ch2.transform(self.X_test)
        self.columners = self.columnie[ch2.get_support()]
        self.ch2=ch2
        self.X_train=X_train
        self.X_test=X_test
        return X_train, X_test

        
    def try_model(self, model, label):
        sexy = model.fit(self.X_train, self.y_train)
        print("Score:", sexy.score(self.X_test, self.y_test))
        print("Cross val score:", cross_val_score(model, self.X_train, self.y_train, cv=5, scoring='f1'))
        yhat = sexy.predict(X_test)
        self.sexy = sexy
        print(classification_report(self.y_test, yhat, target_names=label))
        print(pd.DataFrame(confusion_matrix(self.y_test, yhat), index=[label[0]+'_true', label[1]+'_true'],
                          columns=[label[0]+'_pred',label[1]+'__pred']))
        
    def niceplot(self):
        pass
    
```

```{python}
E = Thipe(X, y_E, rand=45)
```

```{python}
X_train, X_test, y_train, y_test = E.trainy()
X_train, X_test = E.reducy()
E.try_model(model=LogisticRegression(penalty='l1'), label=['Introvert','Extrovert'])
```

```{python}
good_df = pd.DataFrame()
good_df['Features'] = E.columners
good_df['Scores'] = E.ch2.scores_[E.ch2.get_support()]
good_df['p-value'] = E.ch2.pvalues_[E.ch2.get_support()]
good_df
```

```{python}
good_df.sort_values('p-value', ascending=True).head(10)
```

```{python}
lalaland = pd.DataFrame()
lalaland['y'] = y_train
lalaland['x'] = X_train[:,37]
plt.hist(lalaland[lalaland['y']==0]['x'], bins=200, alpha=0.6, color='blue')
plt.hist(lalaland[lalaland['y']==1]['x'], bins=200, alpha=0.6, color='red')
plt.legend(['Introvert','Extrovert'])
plt.title("Distribution of feature 1_15")
```

We first look at how the model performs with logistic regression


This is a classification problem, so of course we cannot just look at the score alone

```{python}
from tpot import TPOTClassifier
tpot = TPOTClassifier(generations=10, population_size=30, verbosity=2, scoring='f1')
tpot.fit(X_train, y_train)
print(tpot.score(X_test, y_test))
tpot.export('tpot_E_try.py')
```

```{python}
S = Thipe(X, y_S)
X_train, X_test, y_train, y_test = S.trainy()
X_train, X_test = S.reducy()
S.try_model(model=LogisticRegression(), label=['Intuitive','Sensing'])
```

```{python}
S.try_model(model=LogisticRegression(penalty='l1'), label=['Intuitive','Sensing'])
```

```{python jupyter={'outputs_hidden': True}}

```

```{python}
tpot.fit(S.X_train, S.y_train)
print(tpot.score(S.X_test, S.y_test))
tpot.export('tpot_S_try.py')
```

```{python}
T = Thipe(X, y_T)
X_train, X_test, y_train, y_test = T.trainy(imbl=False)
X_train, X_test = T.reducy()
T.try_model(model=LogisticRegression(), label=['Feeling','Thinking'])
```

```{python}
T.try_model(model=LogisticRegression(penalty='l1'), label=['Feeling','Thinking'])
```

```{python}
tpot.fit(T.X_train, T.y_train)
print(tpot.score(T.X_test, T.y_test))
tpot.export('tpot_T_try.py')
```

```{python}
J = Thipe(X, y_J)
X_train, X_test, y_train, y_test = J.trainy(imbl=True)
X_train, X_test = J.reducy()
J.try_model(model=LogisticRegression(), label=['Perceiving','Judging'])
```

```{python}
J.try_model(model=LogisticRegression(penalty='l1'), label=['Perceiving','Judging'])
```

```{python}
tpot.fit(J.X_train, J.y_train)
print(tpot.score(J.X_test, J.y_test))
tpot.export('tpot_J_try.py')
```

```{python}
import pickle
# this takes a long time!
fileObject = open('../E.p','wb') 
pickle.dump(E,fileObject)
fileObject.close()
fileObject = open('../S.p','wb') 
pickle.dump(S,fileObject)
fileObject.close()
fileObject = open('../T.p','wb') 
pickle.dump(T,fileObject)
fileObject.close()
fileObject = open('../J.p','wb') 
pickle.dump(J,fileObject)
fileObject.close()
```

Verdict: Sometimes a simple Logistic Regression can do the trick. While using TPOT is convenient for finding out the best possible model to use, it may also run the risk of 'over-processing' and therefore overfitting. As seen from comparison with CV score and test score, the performance is not as great as we think...


## Now comes the half exciting part: Processing brand new data!

After training this data, of course what we really want to do with it is to use the trained model to predict fresh new input.
The class Thipe was used to train the model according to the 4 types, now we should have 4 instances to use.

For each instance, we have:
- 3 x tfidf plus 3 x tsvd
- StandardScaler
- MinMaxScaler
- SelectKBest(Using Chi2)
- Logistic Regression (or whichever model you like)

Process and process, until like from pig until Premium Sausage and Bacon flavoured cup noodles with Tonkotsu broth

```{python}
class NewBerd(object):
    
    def __init__(self):
        self.wordlist=[]
        self.sumdict = {'n_links':0,'n_image':0,'n_video':0,'emoticon_count':0,'mention_count':0,
                        'hashtag_count':0,'mbti_ref_count':0,'ennea_count':0,'bracket_count':0,'dots_count':0,'n_char':0,
                       'n_word':0,'n_action':0,'n_caps':0,'n_caps_char':0}
        self.avgdict={}
        self.taggedcollections=[]
        self.n_posts=0
        self.dff = pd.DataFrame()
        self.columns = []
        self.test=None
    
    #Oh look what I just borrowed!
    
    def extractions_mod(self,line, string):
        """
        Input:
        x = A string of words
        string = regular expression that will match each word

        Output:
        lis = List of weblinks
        lis2 = List of 'words only' posts

        Taken from the preprocessing stage of the training set, except that now I am applying it to the user stage heheh.
        Instead of a list of sentences, we only have one string of words to deal with (each time).
        """
        lis=[]
        lin = line.split()
        sstring = re.compile(string, flags=re.M)
        lis_temp =[]
        for l in lin:
            if sstring.search(l):
                lis.append(sstring.search(l).group(0))
            else:
                lis_temp.append(l)
        lis_string = ' '.join(lis_temp)
        return lis, lis_string
    
    def onelvl_extract(self, x, string):
        """
        Input:
        x = A list of words
        string = regular expression in string form

        How the function works:

        Iterating through each word, if the word matches the regular expression, it will be added into the new list.
        The new list will hence be collecting all the re matched words that came from the input list with the chunk of sentences.
        This function returns the new list.
        """

        lis=[]
        sstring = re.compile(string, flags=re.M)
        for line in x.split():
            if sstring.search(line):
                lis.append(sstring.search(line).group(0))
        return lis
    
    def aggregate(self, words_only, tot_thing, string=''):
        tot_thing = tot_thing + len(self.onelvl_extract(words_only, string=string))
        n_thing = tot_thing * (50./len(self.wordlist))
        return tot_thing, n_thing
    
    def aggregate_web(self, weblink, tot_thing, string='.*'):
        tot_thing = tot_thing + len([y for y in weblink if re.match(string, y)])
        n_thing = tot_thing * (50./len(self.wordlist))
                                    #Special formula to match aggregate number with number of posts in training set
        return tot_thing, n_thing
        
    #Actual function 1
    def preprocess(self, input_string, web=True):
        self.wordlist.append(input_string)
        weblink, words_only = self.extractions_mod(line=input_string, string='https?://.*') #Weblink in list form, words_only in string form
        #Confirm again if we want to use web data
        if web:
            self.sumdict['n_links'], self.avgdict['n_links'] = self.aggregate_web(weblink, self.sumdict['n_links'])
            self.sumdict['n_image'], self.avgdict['n_image'] = self.aggregate_web(weblink, self.sumdict['n_image'],
                                                                   string='.*\.(jpg|png|jpeg|gif).*|.*img.*|.*image.*')
            self.sumdict['n_video'], self.avgdict['n_video'] = self.aggregate_web(weblink, self.sumdict['n_video'],
                                                                   string='https?://.*youtu.*|http.*vimeo.*')
            self.sumdict['n_otherlink'] = self.sumdict['n_links']-self.sumdict['n_image']-self.sumdict['n_video']
            self.avgdict['n_otherlink'] = self.sumdict['n_otherlink'] * (50./len(self.wordlist))
            self.sumdict['mention_count'], self.avgdict['mention_count'] = self.aggregate(words_only,
                                                                           tot_thing=self.sumdict['mention_count'], string='@\w*')
            self.sumdict['hashtag_count'], self.avgdict['hashtag_count'] = self.aggregate(words_only,
                                                                           tot_thing=self.sumdict['hashtag_count'],
                                                                           string='#\w[\w\d]*')
            self.sumdict['mbti_ref_count'], self.avgdict['mbti_ref_count'] = self.aggregate(words_only,
                                                                             tot_thing=self.sumdict['mbti_ref_count'],
                                                                             string='[eiEI][snSN][tfTF][jpJP]')
            self.sumdict['ennea_count'], self.avgdict['ennea_count'] = self.aggregate(words_only,
                                                                       tot_thing=self.sumdict['ennea_count'], string='\dw\d')
            self.sumdict['bracket_count'], self.avgdict['bracket_count'] = self.aggregate(words_only,
                                                                           tot_thing=self.sumdict['bracket_count'],
                                                                           string='\[.*?\]')
        else:
            #Get rid of the dictionary keys not used
            map(lambda x: self.sumdict.pop(x, None), ['n_video','n_links','n_image','mention_count','hashtag_count',
                                                      'mbti_ref_count','ennea_count','bracket_count'])
            
        
        self.sumdict['emoticon_count'], self.avgdict['emoticon_count'] = self.aggregate(words_only,
                                                                         tot_thing=self.sumdict['emoticon_count'], string=':\w*:')
        self.sumdict['dots_count'], self.avgdict['dots_count'] = self.aggregate(words_only,
                                                                tot_thing=self.sumdict['dots_count'], string='\.\.\.')
        self.sumdict['n_action'], self.avgdict['n_action'] = self.aggregate(words_only,
                                                             tot_thing=self.sumdict['n_action'], string='\*\w.*\*')
        #This one abit special
        capp = self.onelvl_extract(words_only, string=r'(?!([eiEI]?[snSN][tfTF][jpJP]|MBTI))[A-Z]{3,}')
        self.sumdict['n_caps'] += len(capp)
        self.avgdict['n_caps'] = self.sumdict['n_caps'] * (50./len(self.wordlist))
        self.sumdict['n_caps_char'] += np.sum([len(y) for y in capp])
        self.avgdict['n_caps_char'] = self.sumdict['n_caps_char'] * (50./len(self.wordlist))
        self.sumdict['n_char'] += len(words_only)
        self.avgdict['n_char'] = self.sumdict['n_char'] * (50./len(self.wordlist))
        self.sumdict['n_word'] += len(words_only.split())
        self.avgdict['n_word'] = self.sumdict['n_word'] * (50./len(self.wordlist))
        #No need sumdict
        self.avgdict['word_cap_ratio'] = float(self.sumdict['n_caps']) / self.sumdict['n_word']
        self.avgdict['char_cap_ratio'] = float(self.sumdict['n_caps_char']) / self.sumdict['n_char']
        self.avgdict['med_char'] = np.median([len(y) for y in self.wordlist])
        self.avgdict['med_word'] = np.median([len(y.split()) for y in self.wordlist])
        self.avgdict['word_char_ratio'] = self.avgdict['med_char'] / self.avgdict['med_word']
        
        #Save to dataframe
        self.dff = pd.DataFrame(self.avgdict, index=[1])
        
        #Create list and dictionary of POS tagging based on existing labels extracted from before
        convtag_dict={'ADJ':['JJ','JJR','JJS'], 'ADP':['EX','TO'], 'ADV':['RB','RBR','RBS','WRB'], 'CONJ':['CC','IN'],'DET':['DT','PDT','WDT'],
              'NOUN':['NN','NNS','NNP','NNPS'], 'NUM':['CD'],'PRT':['RP'],'PRON':['PRP','PRP$','WP','WP$'],
              'VERB':['MD','VB','VBD','VBG','VBN','VBP','VBZ'],'.':['#','$',"''",'(',')',',','.',':'],'X':['FW','LS','UH']}
        tagg = nltk.pos_tag(word_tokenize(input_string))
        self.taggedcollections.append(tagg)
        lollie = ['#','$',"''",'``','(',')',',','.',':','CC','CD','DT','EX','FW','IN','JJ','JJR','JJS','LS','MD','NN','NNS','NNP','NNPS','PDT','POS','PRP','PRP$','RB',
                  'RBR','RBS','RP','SYM','TO','UH','VB','VBD','VBG','VBN','VBP','VBZ','WDT','WP','WP$','WRB']

        #Create columns to get the mean and std for each POS tagging for each row
        for col in lollie:
            self.dff['POS_'+col+'_mean'] = [np.mean([len([y for y in line if y[1] == col])for line in self.taggedcollections])]
            self.dff['POS_'+col+'_std'] = [np.std([len([y for y in line if y[1] == col])for line in self.taggedcollections])]
        for col in convtag_dict.keys():  
            self.dff['BIGPOS_'+col+'_med'] = [np.median([len([y for y in line if y[1] in convtag_dict[col]])
                                                   for line in self.taggedcollections])]
            self.dff['BIGPOS_'+col+'_std']= [np.std([len([y for y in line if y[1] in convtag_dict[col]])
                                                   for line in self.taggedcollections])]

    def perform_magic(self, M):
        """
        This function takes in the instance for each typology class and comes out with the prediction
        Basically the most important function!
        """
        ok_df = pd.DataFrame()
        chunkie = [' '.join(self.wordlist)]
        for i,(tfidf, tsvd) in enumerate(zip(M.tfidf_list, M.tsvd_list)):
            wowwie = tsvd.transform(tfidf.transform(chunkie))
            da_df = pd.DataFrame(wowwie, columns=[str(i+1)+'_'+str(b) for b in np.arange(1,wowwie.shape[1]+1)])
            ok_df = pd.concat([ok_df,da_df], axis=1)
        
        #Set columns
        self.dff = self.dff[M.columns]
        column1 = self.dff.columns
        stded = M.ss.transform(self.dff)
        combinedf = pd.DataFrame(stded, columns=column1).join(ok_df)
        column2 = combinedf.columns
        combinedf = pd.DataFrame(M.mms.transform(combinedf), columns=column2)
        self.test = M.ch2.transform(combinedf)
        magic = M.sexy.predict(self.test)
        return magic
    
    def perform_more_magic(self, M):
        return M.sexy.predict_proba(self.test)[0][M.sexy.predict(self.test)]
    
```

Example input

```{python}
Yixuan = NewBerd()
```

```{python}
entry = 'I dunno lah see how lor...'
Yixuan.preprocess(entry, web=False)
```

```{python}
entry = 'Hey hey you you I know that you like me' #Just change this portion and run from here every time
Yixuan.preprocess(entry, web=False)
```

```{python}
def more_magic(dude):
    typerogy = lambda x, y: y[1] if x[0]==[1] else y[0]
    mpred = []
    mpred_prob = []
    mpred.append(typerogy(dude.perform_magic(E), ['Introvert','Extrovert']))
    mpred.append(typerogy(dude.perform_magic(S), ['iNtuitive','Sensing']))
    mpred.append(typerogy(dude.perform_magic(T), ['Feeling','Thinking']))
    mpred.append(typerogy(dude.perform_magic(J), ['Perceiving','Judging']))
    for m in [E, S, T, J]:
        mpred_prob.append(dude.perform_more_magic(m))
    tadaa = pd.DataFrame()
    tadaa['Type'] = mpred
    tadaa['Probability'] = mpred_prob
    return tadaa
```

```{python}
more_magic(Yixuan)
```

```{python}
Yixuan.wordlist
```

```{python}
E.sexy.predict_proba(Yixuan.test)[0][0]
```

Tentative test case: Read from own dataset

```{python}
#Choose your index from 0 to 8674
select = 8
Someguy = NewBerd()
```

```{python}
mbti_test = pd.read_csv('mbti_1.csv')
mbti_textlist = mbti_test.iloc[select,1].split('|||')
#print mbti_textlist
for line in mbti_textlist:
    Someguy.preprocess(line, web=False)
```

```{python}
print(mbti_test.iloc[select,0])
# %time print(more_magic(Someguy))
```

The next exciting part will be myself building a webapp or even a telegram bot or something. Stay tuned!
